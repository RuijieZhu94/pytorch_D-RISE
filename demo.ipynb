{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU torch==1.7.1 torchvision==0.8.2\n",
    "!pip install -q mmcv-full==1.3.8 -f https://download.openmmlab.com/mmcv/dist/cu102/torch1.7.0/index.html\n",
    "!pip install -q mmdet==2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import init_detector, inference_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -q https://github.com/hysts/pytorch_D-RISE\n",
    "!wget -q https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'pytorch_D-RISE/mmdet_configs/configs/faster_rcnn/faster_rcnn_r50_fpn_2x_coco.py'\n",
    "checkpoint = 'faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_detector(config, checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    'person',\n",
    "    'bicycle',\n",
    "    'car',\n",
    "    'motorcycle',\n",
    "    'airplane',\n",
    "    'bus',\n",
    "    'train',\n",
    "    'truck',\n",
    "    'boat',\n",
    "    'traffic light',\n",
    "    'fire hydrant',\n",
    "    'stop sign',\n",
    "    'parking meter',\n",
    "    'bench',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'dog',\n",
    "    'horse',\n",
    "    'sheep',\n",
    "    'cow',\n",
    "    'elephant',\n",
    "    'bear',\n",
    "    'zebra',\n",
    "    'giraffe',\n",
    "    'backpack',\n",
    "    'umbrella',\n",
    "    'handbag',\n",
    "    'tie',\n",
    "    'suitcase',\n",
    "    'frisbee',\n",
    "    'skis',\n",
    "    'snowboard',\n",
    "    'sports ball',\n",
    "    'kite',\n",
    "    'baseball bat',\n",
    "    'baseball glove',\n",
    "    'skateboard',\n",
    "    'surfboard',\n",
    "    'tennis racket',\n",
    "    'bottle',\n",
    "    'wine glass',\n",
    "    'cup',\n",
    "    'fork',\n",
    "    'knife',\n",
    "    'spoon',\n",
    "    'bowl',\n",
    "    'banana',\n",
    "    'apple',\n",
    "    'sandwich',\n",
    "    'orange',\n",
    "    'broccoli',\n",
    "    'carrot',\n",
    "    'hot dog',\n",
    "    'pizza',\n",
    "    'donut',\n",
    "    'cake',\n",
    "    'chair',\n",
    "    'couch',\n",
    "    'potted plant',\n",
    "    'bed',\n",
    "    'dining table',\n",
    "    'toilet',\n",
    "    'tv',\n",
    "    'laptop',\n",
    "    'mouse',\n",
    "    'remote',\n",
    "    'keyboard',\n",
    "    'cell phone',\n",
    "    'microwave',\n",
    "    'oven',\n",
    "    'toaster',\n",
    "    'sink',\n",
    "    'refrigerator',\n",
    "    'book',\n",
    "    'clock',\n",
    "    'vase',\n",
    "    'scissors',\n",
    "    'teddy bear',\n",
    "    'hair drier',\n",
    "    'toothbrush',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('pytorch_D-RISE/images/image00.jpg')\n",
    "scale = 600 / min(image.shape[:2])\n",
    "image = cv2.resize(image,\n",
    "                   None,\n",
    "                   fx=scale,\n",
    "                   fy=scale,\n",
    "                   interpolation=cv2.INTER_AREA)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = inference_detector(model, image)\n",
    "res = image.copy()\n",
    "for i, pred in enumerate(out):\n",
    "    for *box, score in pred:\n",
    "        if score < 0.5:\n",
    "            break\n",
    "        box = tuple(np.round(box).astype(int).tolist())\n",
    "        print(i, label_names[i], box, score)\n",
    "        cv2.rectangle(res, box[:2], box[2:], (0, 255, 0), 5)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(res[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(image_size, grid_size, prob_thresh):\n",
    "    image_w, image_h = image_size\n",
    "    grid_w, grid_h = grid_size\n",
    "    cell_w, cell_h = image_w // grid_w, image_h // grid_h\n",
    "    up_w, up_h = (grid_w + 1) * cell_w, (grid_h + 1) * cell_h\n",
    "\n",
    "    mask = (np.random.uniform(0, 1, size=(grid_h, grid_w)) <\n",
    "            prob_thresh).astype(np.float32)\n",
    "    mask = cv2.resize(mask, (up_w, up_h), interpolation=cv2.INTER_LINEAR)\n",
    "    offset_w = np.random.randint(0, up_w - image_w)\n",
    "    offset_h = np.random.randint(0, up_h - image_h)\n",
    "    mask = mask[offset_h:offset_h + image_h, offset_w:offset_w + image_w]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_image(image, mask):\n",
    "    masked = ((image.astype(np.float32) / 255 * np.dstack([mask] * 3)) *\n",
    "              255).astype(np.uint8)\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "image_h, image_w = image.shape[:2]\n",
    "\n",
    "images = []\n",
    "for _ in range(25):\n",
    "    mask = generate_mask(image_size=(image_w, image_h),\n",
    "                         grid_size=(16, 16),\n",
    "                         prob_thresh=0.5)\n",
    "    masked = mask_image(image, mask)\n",
    "    out = inference_detector(model, masked)\n",
    "    res = masked.copy()\n",
    "    for pred in out:\n",
    "        for *box, score in pred:\n",
    "            if score < 0.5:\n",
    "                break\n",
    "            box = tuple(np.round(box).astype(int).tolist())\n",
    "            cv2.rectangle(res, box[:2], box[2:], (0, 255, 0), 5)\n",
    "    images.append(res)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "axes = fig.subplots(5, 5)\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axes[i][j].imshow(images[i * 5 + j][:, :, ::-1])\n",
    "        axes[i][j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    box1 = np.asarray(box1)\n",
    "    box2 = np.asarray(box2)\n",
    "    tl = np.vstack([box1[:2], box2[:2]]).max(axis=0)\n",
    "    br = np.vstack([box1[2:], box2[2:]]).min(axis=0)\n",
    "    intersection = np.prod(br - tl) * np.all(tl < br).astype(float)\n",
    "    area1 = np.prod(box1[2:] - box1[:2])\n",
    "    area2 = np.prod(box2[2:] - box2[:2])\n",
    "    return intersection / (area1 + area2 - intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency_map(image,\n",
    "                          target_class_index,\n",
    "                          target_box,\n",
    "                          prob_thresh=0.5,\n",
    "                          grid_size=(16, 16),\n",
    "                          n_masks=5000,\n",
    "                          seed=0):\n",
    "    np.random.seed(seed)\n",
    "    image_h, image_w = image.shape[:2]\n",
    "    res = np.zeros((image_h, image_w), dtype=np.float32)\n",
    "    for _ in tqdm.notebook.tqdm(range(n_masks)):\n",
    "        mask = generate_mask(image_size=(image_w, image_h),\n",
    "                             grid_size=grid_size,\n",
    "                             prob_thresh=prob_thresh)\n",
    "        masked = mask_image(image, mask)\n",
    "        out = inference_detector(model, masked)\n",
    "        pred = out[target_class_index]\n",
    "        score = max([iou(target_box, box) * score for *box, score in pred],\n",
    "                    default=0)\n",
    "        res += mask * score\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_box = np.array([289, 72, 491, 388])\n",
    "saliency_map = generate_saliency_map(image,\n",
    "                                     target_class_index=15,\n",
    "                                     target_box=target_box,\n",
    "                                     prob_thresh=0.5,\n",
    "                                     grid_size=(16, 16),\n",
    "                                     n_masks=1000)\n",
    "\n",
    "image_with_bbox = image.copy()\n",
    "cv2.rectangle(image_with_bbox, tuple(target_box[:2]), tuple(target_box[2:]),\n",
    "              (0, 255, 0), 5)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image_with_bbox[:, :, ::-1])\n",
    "plt.imshow(saliency_map, cmap='jet', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('pytorch_D-RISE/images/image01.jpg')\n",
    "scale = 600 / min(image.shape[:2])\n",
    "image = cv2.resize(image,\n",
    "                   None,\n",
    "                   fx=scale,\n",
    "                   fy=scale,\n",
    "                   interpolation=cv2.INTER_AREA)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = inference_detector(model, image)\n",
    "res = image.copy()\n",
    "for i, pred in enumerate(out):\n",
    "    for *box, score in pred:\n",
    "        if score < 0.5:\n",
    "            break\n",
    "        box = tuple(np.round(box).astype(int).tolist())\n",
    "        print(i, label_names[i], box, score)\n",
    "        cv2.rectangle(res, box[:2], box[2:], (0, 255, 0), 5)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(res[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_box = np.array([225, 313, 472, 806])\n",
    "saliency_map = generate_saliency_map(image,\n",
    "                                     target_class_index=15,\n",
    "                                     target_box=target_box,\n",
    "                                     prob_thresh=0.5,\n",
    "                                     grid_size=(16, 16),\n",
    "                                     n_masks=1000)\n",
    "\n",
    "image_with_bbox = image.copy()\n",
    "cv2.rectangle(image_with_bbox, tuple(target_box[:2]), tuple(target_box[2:]),\n",
    "              (0, 255, 0), 5)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image_with_bbox[:, :, ::-1])\n",
    "plt.imshow(saliency_map, cmap='jet', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_box = np.array([225, 313, 472, 806])\n",
    "saliency_map = generate_saliency_map(image,\n",
    "                                     target_class_index=15,\n",
    "                                     target_box=target_box,\n",
    "                                     prob_thresh=0.3,\n",
    "                                     grid_size=(16, 16),\n",
    "                                     n_masks=1000)\n",
    "\n",
    "image_with_bbox = image.copy()\n",
    "cv2.rectangle(image_with_bbox, tuple(target_box[:2]), tuple(target_box[2:]),\n",
    "              (0, 255, 0), 5)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image_with_bbox[:, :, ::-1])\n",
    "plt.imshow(saliency_map, cmap='jet', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_box = np.array([225, 313, 472, 806])\n",
    "saliency_map = generate_saliency_map(image,\n",
    "                                     target_class_index=15,\n",
    "                                     target_box=target_box,\n",
    "                                     prob_thresh=0.2,\n",
    "                                     grid_size=(16, 16),\n",
    "                                     n_masks=1000)\n",
    "\n",
    "image_with_bbox = image.copy()\n",
    "cv2.rectangle(image_with_bbox, tuple(target_box[:2]), tuple(target_box[2:]),\n",
    "              (0, 255, 0), 5)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image_with_bbox[:, :, ::-1])\n",
    "plt.imshow(saliency_map, cmap='jet', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_box = np.array([246, 478, 430, 576])\n",
    "saliency_map = generate_saliency_map(image,\n",
    "                                     target_class_index=27,\n",
    "                                     target_box=target_box,\n",
    "                                     prob_thresh=0.5,\n",
    "                                     grid_size=(16, 16),\n",
    "                                     n_masks=1000)\n",
    "\n",
    "image_with_bbox = image.copy()\n",
    "cv2.rectangle(image_with_bbox, tuple(target_box[:2]), tuple(target_box[2:]),\n",
    "              (0, 255, 0), 5)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image_with_bbox[:, :, ::-1])\n",
    "plt.imshow(saliency_map, cmap='jet', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('pytorch_D-RISE/images/image02.jpg')\n",
    "scale = 600 / min(image.shape[:2])\n",
    "image = cv2.resize(image,\n",
    "                   None,\n",
    "                   fx=scale,\n",
    "                   fy=scale,\n",
    "                   interpolation=cv2.INTER_AREA)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = inference_detector(model, image)\n",
    "res = image.copy()\n",
    "for i, pred in enumerate(out):\n",
    "    for *box, score in pred:\n",
    "        if score < 0.5:\n",
    "            break\n",
    "        box = tuple(np.round(box).astype(int).tolist())\n",
    "        print(i, label_names[i], box, score)\n",
    "        cv2.rectangle(res, box[:2], box[2:], (0, 255, 0), 5)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(res[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_box = np.array([20, 458, 373, 555])\n",
    "saliency_map = generate_saliency_map(image,\n",
    "                                     target_class_index=71,\n",
    "                                     target_box=target_box,\n",
    "                                     prob_thresh=0.3,\n",
    "                                     grid_size=(16, 16),\n",
    "                                     n_masks=1000)\n",
    "\n",
    "image_with_bbox = image.copy()\n",
    "cv2.rectangle(image_with_bbox, tuple(target_box[:2]), tuple(target_box[2:]),\n",
    "              (0, 255, 0), 5)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image_with_bbox[:, :, ::-1])\n",
    "plt.imshow(saliency_map, cmap='jet', alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-annotation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-circular",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-routine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
